{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyuial1RQJNI",
        "outputId": "1745c6e6-87c9-47d8-e9c9-b62ecb3941d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (1035, 1882)\n",
            "X_val   shape: (222, 1882)\n",
            "X_test  shape: (222, 1882)\n",
            "Unseen portion saved as 'unseen_data.csv' with shape: (164, 10)\n",
            "Done building features and saving 'unseen_data.csv'!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('./cleaned_data_combined.csv')\n",
        "new_column_names = {\n",
        "    \"Q1: From a scale 1 to 5, how complex is it to make this food? (Where 1 is the most simple, and 5 is the most complex)\": \"1\",\n",
        "    \"Q2: How many ingredients would you expect this food item to contain?\": \"2\",\n",
        "    \"Q3: In what setting would you expect this food to be served? Please check all that apply\": \"3\",\n",
        "    \"Q4: How much would you expect to pay for one serving of this food item?\": \"4\",\n",
        "    \"Q5: What movie do you think of when thinking of this food item?\": \"5\",\n",
        "    \"Q6: What drink would you pair with this food item?\": \"6\",\n",
        "    \"Q7: When you think about this food item, who does it remind you of?\": \"7\",\n",
        "    \"Q8: How much hot sauce would you add to this food item?\": \"8\",\n",
        "    \"Label\": \"label\"\n",
        "}\n",
        "df = df.rename(columns=new_column_names)\n",
        "\n",
        "N = len(df)\n",
        "unseen_size = int(0.1 * N)\n",
        "indices = np.arange(N)\n",
        "\n",
        "np.random.seed(42)\n",
        "shuffled_idxs = np.random.permutation(indices)\n",
        "unseen_idxs = shuffled_idxs[:unseen_size]\n",
        "rest_idxs   = shuffled_idxs[unseen_size:]\n",
        "\n",
        "# Save the unseen portion to a separate CSV\n",
        "df_unseen = df.iloc[unseen_idxs].copy()\n",
        "df_unseen.to_csv(\"unseen_data.csv\", index=False)\n",
        "\n",
        "df_clean = df.iloc[rest_idxs].copy()\n",
        "\n",
        "def build_vocab(series):\n",
        "    series = series.astype(str).str.lower().str.replace(r'[^a-z0-9 ]', '', regex=True)\n",
        "    vocab = set()\n",
        "    for entry in series.dropna():\n",
        "        words = str(entry).split()\n",
        "        vocab.update(words)\n",
        "    return list(vocab)\n",
        "\n",
        "def make_bow(series, vocab):\n",
        "    X = np.zeros((len(series), len(vocab)), dtype=int)\n",
        "    vocab_dict = {word: j for j, word in enumerate(vocab)}\n",
        "    for i, entry in enumerate(series):\n",
        "        words = set(str(entry).lower().split())\n",
        "        for word in words:\n",
        "            if word in vocab_dict:\n",
        "                X[i, vocab_dict[word]] = 1\n",
        "    return X\n",
        "\n",
        "df_clean = df_clean.dropna(subset=['4']).copy()\n",
        "\n",
        "df_clean['1'] = df_clean['1'].astype(int)\n",
        "\n",
        "vocab_q2 = build_vocab(df_clean['2'])\n",
        "vocab_q4 = build_vocab(df_clean['4'])\n",
        "vocab_q5 = build_vocab(df_clean['5'])\n",
        "vocab_q6 = build_vocab(df_clean['6'])\n",
        "\n",
        "X_q2 = make_bow(df_clean['2'], vocab_q2)\n",
        "X_q4 = make_bow(df_clean['4'], vocab_q4)\n",
        "X_q5 = make_bow(df_clean['5'], vocab_q5)\n",
        "X_q6 = make_bow(df_clean['6'], vocab_q6)\n",
        "\n",
        "X_q3 = df_clean['3'].str.get_dummies(sep=\",\")\n",
        "expected_q3_columns = list(X_q3.columns)\n",
        "\n",
        "X_q7 = df_clean['7'].str.get_dummies(sep=\",\")\n",
        "expected_q7_columns = list(X_q7.columns)\n",
        "\n",
        "df_clean['8'] = df_clean['8'].astype(\"category\").cat.codes\n",
        "\n",
        "df_clean['label'] = df_clean['label'].astype(\"category\").cat.codes\n",
        "y = df_clean['label'].values\n",
        "\n",
        "features = np.hstack([\n",
        "    df_clean['1'].values.reshape(-1,1),\n",
        "    X_q2,\n",
        "    X_q3.values,\n",
        "    X_q4,\n",
        "    X_q5,\n",
        "    X_q6,\n",
        "    X_q7.values,\n",
        "    df_clean['8'].values.reshape(-1,1)\n",
        "])\n",
        "\n",
        "std_devs = features.std(axis=0)\n",
        "non_constant_cols = (std_devs != 0)\n",
        "features = features[:, non_constant_cols]\n",
        "\n",
        "features = np.nan_to_num(features)\n",
        "\n",
        "X_mean = features.mean(axis=0)\n",
        "X_std  = features.std(axis=0)\n",
        "epsilon_norm = 1e-8\n",
        "features = (features - X_mean)/(X_std + epsilon_norm)\n",
        "\n",
        "N_rest = len(features)\n",
        "idx2 = np.random.permutation(N_rest)\n",
        "train_end = int(0.7 * N_rest)\n",
        "val_end   = int(0.85 * N_rest)\n",
        "\n",
        "X_train = features[idx2[:train_end]]\n",
        "y_train = y[idx2[:train_end]]\n",
        "\n",
        "X_val = features[idx2[train_end:val_end]]\n",
        "y_val = y[idx2[train_end:val_end]]\n",
        "\n",
        "X_test = features[idx2[val_end:]]\n",
        "y_test = y[idx2[val_end:]]\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val   shape:\", X_val.shape)\n",
        "print(\"X_test  shape:\", X_test.shape)\n",
        "print(\"Unseen portion saved as 'unseen_data.csv' with shape:\", df_unseen.shape)\n",
        "\n",
        "np.savez(\"vocabularies.npz\",\n",
        "    vocab_q2=np.array(vocab_q2, dtype=object),\n",
        "    vocab_q4=np.array(vocab_q4, dtype=object),\n",
        "    vocab_q5=np.array(vocab_q5, dtype=object),\n",
        "    vocab_q6=np.array(vocab_q6, dtype=object),\n",
        "    expected_q3_columns=np.array(expected_q3_columns, dtype=object),\n",
        "    expected_q7_columns=np.array(expected_q7_columns, dtype=object)\n",
        ")\n",
        "\n",
        "np.savez(\"mlp_params.npz\",\n",
        "    non_constant_columns=non_constant_cols,\n",
        "    X_mean=X_mean,\n",
        "    X_std=X_std\n",
        ")\n",
        "\n",
        "print(\"Done building features and saving 'unseen_data.csv'!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H33MWtq03e7B",
        "outputId": "5381cfa7-5539-46d5-be4d-37f90bc31af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training multi-class logistic regression...\n",
            "Epoch 0/2000, Loss=1.098612\n",
            "Epoch 100/2000, Loss=0.354030\n",
            "Epoch 200/2000, Loss=0.232913\n",
            "Epoch 300/2000, Loss=0.182194\n",
            "Epoch 400/2000, Loss=0.153468\n",
            "Epoch 500/2000, Loss=0.134570\n",
            "Epoch 600/2000, Loss=0.120983\n",
            "Epoch 700/2000, Loss=0.110630\n",
            "Epoch 800/2000, Loss=0.102408\n",
            "Epoch 900/2000, Loss=0.095676\n",
            "Epoch 1000/2000, Loss=0.090033\n",
            "Epoch 1100/2000, Loss=0.085213\n",
            "Epoch 1200/2000, Loss=0.081032\n",
            "Epoch 1300/2000, Loss=0.077359\n",
            "Epoch 1400/2000, Loss=0.074099\n",
            "Epoch 1500/2000, Loss=0.071179\n",
            "Epoch 1600/2000, Loss=0.068542\n",
            "Epoch 1700/2000, Loss=0.066145\n",
            "Epoch 1800/2000, Loss=0.063953\n",
            "Epoch 1900/2000, Loss=0.061939\n",
            "Train Accuracy: 0.991304347826087\n",
            "Validation Accuracy: 0.8783783783783784\n",
            "Test Accuracy: 0.8918918918918919\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    shifted = z - np.max(z, axis=1, keepdims=True)\n",
        "    exps = np.exp(shifted)\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    eps = 1e-15\n",
        "    n = y_true.shape[0]\n",
        "    clipped = np.clip(y_pred, eps, 1 - eps)\n",
        "    y_one_hot = np.zeros_like(y_pred)\n",
        "    y_one_hot[np.arange(n), y_true] = 1\n",
        "    return -np.sum(y_one_hot * np.log(clipped)) / n\n",
        "\n",
        "def train_multiclass_logreg(X_train, y_train, lr=0.01, epochs=1000, reg=0.0):\n",
        "    n_samples, n_features = X_train.shape\n",
        "    n_classes = len(np.unique(y_train))\n",
        "\n",
        "    X_bias = np.hstack((np.ones((n_samples, 1)), X_train))\n",
        "\n",
        "    W = np.zeros((n_features + 1, n_classes))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        logits = X_bias @ W  # shape: (n_samples, n_classes)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        loss = cross_entropy_loss(y_train, probs)\n",
        "        if reg > 0.0:\n",
        "            loss += 0.5 * reg * np.sum(W[1:]**2)  # exclude bias term\n",
        "\n",
        "        y_one_hot = np.zeros_like(probs)\n",
        "        y_one_hot[np.arange(n_samples), y_train] = 1\n",
        "        grad = X_bias.T @ (probs - y_one_hot) / n_samples  # shape: (n_features+1, n_classes)\n",
        "        if reg > 0.0:\n",
        "            grad[1:] += reg * W[1:]  # exclude bias term\n",
        "\n",
        "        W -= lr * grad\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}/{epochs}, Loss={loss:.6f}\")\n",
        "\n",
        "    return W\n",
        "\n",
        "def predict_multiclass_logreg(X, W):\n",
        "    n_samples = X.shape[0]\n",
        "    X_bias = np.hstack((np.ones((n_samples,1)), X))\n",
        "    logits = X_bias @ W\n",
        "    probs = softmax(logits)\n",
        "    return np.argmax(probs, axis=1)\n",
        "\n",
        "\n",
        "print(\"Training multi-class logistic regression...\")\n",
        "W_trained = train_multiclass_logreg(X_train, y_train, lr=0.01, epochs=2000, reg=0.0)\n",
        "\n",
        "train_preds = predict_multiclass_logreg(X_train, W_trained)\n",
        "train_acc = np.mean(train_preds == y_train)\n",
        "print(\"Train Accuracy:\", train_acc)\n",
        "\n",
        "val_preds = predict_multiclass_logreg(X_val, W_trained)\n",
        "val_acc = np.mean(val_preds == y_val)\n",
        "print(\"Validation Accuracy:\", val_acc)\n",
        "\n",
        "test_preds = predict_multiclass_logreg(X_test, W_trained)\n",
        "test_acc = np.mean(test_preds == y_test)\n",
        "print(\"Test Accuracy:\", test_acc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZcHro9Z897y"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "JhgioZprZdCm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting DecisionTree training (multi-class)...\n",
            "DecisionTree Train Acc: 0.8966183574879227\n",
            "DecisionTree Test Acc: 0.7927927927927928\n"
          ]
        }
      ],
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature   = feature\n",
        "        self.threshold = threshold\n",
        "        self.left      = left\n",
        "        self.right     = right\n",
        "        self.value     = value\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, criterion='gini'):\n",
        "        self.max_depth         = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf  = min_samples_leaf\n",
        "        self.criterion         = criterion\n",
        "        self.root = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.root = self._build_tree(X, y, depth=0)\n",
        "\n",
        "    def _build_tree(self, X, y, depth):\n",
        "        if len(set(y))==1:\n",
        "            return Node(value=y[0])\n",
        "        if self.max_depth and depth>=self.max_depth:\n",
        "            return Node(value=max(set(y), key=list(y).count))\n",
        "        if len(y) < self.min_samples_split:\n",
        "            return Node(value=max(set(y), key=list(y).count))\n",
        "\n",
        "        best_feat, best_thresh = self._best_split(X, y)\n",
        "        if best_feat is None:\n",
        "            return Node(value=max(set(y), key=list(y).count))\n",
        "\n",
        "        left_idx = [i for i in range(len(X)) if X[i][best_feat]<best_thresh]\n",
        "        right_idx= [i for i in range(len(X)) if X[i][best_feat]>=best_thresh]\n",
        "\n",
        "        if len(left_idx)<self.min_samples_leaf or len(right_idx)<self.min_samples_leaf:\n",
        "            return Node(value=max(set(y), key=list(y).count))\n",
        "\n",
        "        left_child  = self._build_tree([X[i] for i in left_idx],\n",
        "                                       [y[i] for i in left_idx],\n",
        "                                       depth+1)\n",
        "        right_child = self._build_tree([X[i] for i in right_idx],\n",
        "                                       [y[i] for i in right_idx],\n",
        "                                       depth+1)\n",
        "        return Node(best_feat, best_thresh, left_child, right_child)\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        best_gini = float('inf')\n",
        "        best_feat = None\n",
        "        best_thr  = None\n",
        "        for feat in range(len(X[0])):\n",
        "            all_vals = set(row[feat] for row in X)\n",
        "            for t in all_vals:\n",
        "                l_y = [y[i] for i in range(len(X)) if X[i][feat]<t]\n",
        "                r_y = [y[i] for i in range(len(X)) if X[i][feat]>=t]\n",
        "                g   = self._gini_impurity(l_y, r_y)\n",
        "                if g<best_gini:\n",
        "                    best_gini = g\n",
        "                    best_feat = feat\n",
        "                    best_thr  = t\n",
        "        return best_feat, best_thr\n",
        "\n",
        "    def _gini_impurity(self, ly, ry):\n",
        "        def gini(arr):\n",
        "            cnts = [arr.count(c) for c in set(arr)]\n",
        "            total= len(arr)\n",
        "            return 1 - sum((c/total)**2 for c in cnts)\n",
        "        n = len(ly)+len(ry)\n",
        "        gl= gini(ly)\n",
        "        gr= gini(ry)\n",
        "        return (len(ly)/n)*gl + (len(ry)/n)*gr\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict_sample(x, self.root) for x in X]\n",
        "\n",
        "    def _predict_sample(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature]<node.threshold:\n",
        "            return self._predict_sample(x, node.left)\n",
        "        return self._predict_sample(x, node.right)\n",
        "\n",
        "    def accuracy(self, X, y):\n",
        "        preds = self.predict(X)\n",
        "        return np.mean([p==t for p,t in zip(preds,y)])\n",
        "\n",
        "print(\"Starting DecisionTree training (multi-class)...\")\n",
        "dt = DecisionTree(max_depth=10, min_samples_split=10, min_samples_leaf=2, criterion='gini')\n",
        "dt.fit(X_train.tolist(), y_train.tolist())\n",
        "train_acc_dt = dt.accuracy(X_train.tolist(), y_train.tolist())\n",
        "test_acc_dt  = dt.accuracy(X_test.tolist(),  y_test.tolist())\n",
        "print(\"DecisionTree Train Acc:\", train_acc_dt)\n",
        "print(\"DecisionTree Test Acc:\",  test_acc_dt)\n",
        "np.savez(\"decision_tree.npz\", tree=dt.root)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAp9O4Zc5q2R",
        "outputId": "06181b90-ecfb-464d-f097-91887739fa73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting multi-class MLP training...\n",
            "Iteration 0/200, Loss=0.320512\n",
            "Iteration 10/200, Loss=0.014655\n",
            "Iteration 20/200, Loss=0.004611\n",
            "Iteration 30/200, Loss=0.002205\n",
            "Iteration 40/200, Loss=0.001250\n",
            "Iteration 50/200, Loss=0.000796\n",
            "Iteration 60/200, Loss=0.000534\n",
            "Iteration 70/200, Loss=0.000381\n",
            "Iteration 80/200, Loss=0.000279\n",
            "Iteration 90/200, Loss=0.000209\n",
            "Iteration 100/200, Loss=0.000160\n",
            "Iteration 110/200, Loss=0.000124\n",
            "Iteration 120/200, Loss=0.000098\n",
            "Iteration 130/200, Loss=0.000078\n",
            "Iteration 140/200, Loss=0.000062\n",
            "Iteration 150/200, Loss=0.000050\n",
            "Iteration 160/200, Loss=0.000040\n",
            "Iteration 170/200, Loss=0.000032\n",
            "Iteration 180/200, Loss=0.000026\n",
            "Iteration 190/200, Loss=0.000022\n",
            "Train Accuracy: 1.0\n",
            "Val Accuracy: 0.8063063063063063\n",
            "Test Accuracy: 0.8243243243243243\n"
          ]
        }
      ],
      "source": [
        "class MLPClassifier:\n",
        "    def __init__(self, hidden_layer_sizes=(150,), activation='relu', solver='adam',\n",
        "                 max_iter=200, learning_rate=0.001, batch_size=32, random_state=None,\n",
        "                 beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.hidden_layer_sizes = hidden_layer_sizes\n",
        "        self.activation = activation\n",
        "        self.solver = solver\n",
        "        self.max_iter = max_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        if random_state is not None:\n",
        "            np.random.seed(random_state)\n",
        "\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.n_layers = len(hidden_layer_sizes) + 1\n",
        "        self.n_classes = None\n",
        "        self.initialized = False\n",
        "\n",
        "        self.m_weights = []\n",
        "        self.v_weights = []\n",
        "        self.m_biases  = []\n",
        "        self.v_biases  = []\n",
        "        self.t = 0\n",
        "\n",
        "    def _initialize_parameters(self, n_features, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        layer_sizes = [n_features] + list(self.hidden_layer_sizes) + [n_classes]\n",
        "        for i in range(1, len(layer_sizes)):\n",
        "            scale = np.sqrt(2.0 / layer_sizes[i-1])\n",
        "            W = np.random.randn(layer_sizes[i-1], layer_sizes[i]) * scale\n",
        "            b = np.zeros(layer_sizes[i])\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "            if self.solver == 'adam':\n",
        "                self.m_weights.append(np.zeros_like(W))\n",
        "                self.v_weights.append(np.zeros_like(W))\n",
        "                self.m_biases.append(np.zeros_like(b))\n",
        "                self.v_biases.append(np.zeros_like(b))\n",
        "        self.initialized = True\n",
        "\n",
        "    def _activation_function(self, z, derivative=False):\n",
        "        if self.activation == 'relu':\n",
        "            if not derivative:\n",
        "                return np.maximum(0, z)\n",
        "            return (z > 0).astype(float)\n",
        "        elif self.activation == 'tanh':\n",
        "            if not derivative:\n",
        "                return np.tanh(z)\n",
        "            return 1 - np.tanh(z)**2\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation function\")\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        exps = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "    def _forward_pass(self, X):\n",
        "        activations = [X]\n",
        "        pre_acts    = []\n",
        "        for i in range(self.n_layers - 1):\n",
        "            z = activations[-1] @ self.weights[i] + self.biases[i]\n",
        "            pre_acts.append(z)\n",
        "            a = self._activation_function(z)\n",
        "            activations.append(a)\n",
        "        z_out = activations[-1] @ self.weights[-1] + self.biases[-1]\n",
        "        pre_acts.append(z_out)\n",
        "        out = self._softmax(z_out)\n",
        "        activations.append(out)\n",
        "        return activations, pre_acts\n",
        "\n",
        "    def _compute_cost(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        y_true_1hot = np.zeros((m, self.n_classes))\n",
        "        y_true_1hot[np.arange(m), y_true] = 1\n",
        "        eps = 1e-15\n",
        "        logp = -np.log(np.maximum(y_pred, eps)) * y_true_1hot\n",
        "        return np.sum(logp)/m\n",
        "\n",
        "    def _backpropagation(self, X, y, activations, pre_acts):\n",
        "        m = X.shape[0]\n",
        "        y_1hot = np.zeros((m, self.n_classes))\n",
        "        y_1hot[np.arange(m), y] = 1\n",
        "\n",
        "        dW = [None]*self.n_layers\n",
        "        db = [None]*self.n_layers\n",
        "\n",
        "        delta = activations[-1] - y_1hot\n",
        "        dW[-1] = activations[-2].T @ delta / m\n",
        "        db[-1] = np.mean(delta, axis=0)\n",
        "\n",
        "        for i in range(self.n_layers - 2, -1, -1):\n",
        "            delta = (delta @ self.weights[i+1].T) * self._activation_function(pre_acts[i], derivative=True)\n",
        "            dW[i] = activations[i].T @ delta / m\n",
        "            db[i] = np.mean(delta, axis=0)\n",
        "        return dW, db\n",
        "\n",
        "    def _update_params(self, dW, db):\n",
        "        self.t += 1\n",
        "        if self.solver == 'sgd':\n",
        "            for i in range(self.n_layers):\n",
        "                self.weights[i] -= self.learning_rate * dW[i]\n",
        "                self.biases[i]  -= self.learning_rate * db[i]\n",
        "        elif self.solver == 'adam':\n",
        "            for i in range(self.n_layers):\n",
        "                self.m_weights[i] = self.beta1*self.m_weights[i] + (1-self.beta1)*dW[i]\n",
        "                self.v_weights[i] = self.beta2*self.v_weights[i] + (1-self.beta2)*(dW[i]**2)\n",
        "\n",
        "                mw_corr = self.m_weights[i]/(1-self.beta1**self.t)\n",
        "                vw_corr = self.v_weights[i]/(1-self.beta2**self.t)\n",
        "                self.weights[i] -= self.learning_rate*mw_corr/(np.sqrt(vw_corr)+self.epsilon)\n",
        "\n",
        "                self.m_biases[i] = self.beta1*self.m_biases[i] + (1-self.beta1)*db[i]\n",
        "                self.v_biases[i] = self.beta2*self.v_biases[i] + (1-self.beta2)*(db[i]**2)\n",
        "\n",
        "                mb_corr = self.m_biases[i]/(1-self.beta1**self.t)\n",
        "                vb_corr = self.v_biases[i]/(1-self.beta2**self.t)\n",
        "                self.biases[i] -= self.learning_rate*mb_corr/(np.sqrt(vb_corr)+self.epsilon)\n",
        "\n",
        "    def _create_mini_batches(self, X, y):\n",
        "        m = X.shape[0]\n",
        "        idxs = np.random.permutation(m)\n",
        "        X_shuf = X[idxs]\n",
        "        y_shuf = y[idxs]\n",
        "        n_full = m//self.batch_size\n",
        "        mini_batches = []\n",
        "        for i in range(n_full):\n",
        "            start = i*self.batch_size\n",
        "            end   = (i+1)*self.batch_size\n",
        "            mini_batches.append((X_shuf[start:end], y_shuf[start:end]))\n",
        "        if m%self.batch_size !=0:\n",
        "            start = n_full*self.batch_size\n",
        "            mini_batches.append((X_shuf[start:], y_shuf[start:]))\n",
        "        return mini_batches\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "        if not self.initialized:\n",
        "            self._initialize_parameters(n_features, n_classes)\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            batches = self._create_mini_batches(X, y)\n",
        "            for (Xb, yb) in batches:\n",
        "                acts, pre_acts = self._forward_pass(Xb)\n",
        "                dW, db = self._backpropagation(Xb, yb, acts, pre_acts)\n",
        "                self._update_params(dW, db)\n",
        "            if i%10==0:\n",
        "                acts_full, _ = self._forward_pass(X)\n",
        "                loss = self._compute_cost(y, acts_full[-1])\n",
        "                print(f\"Iteration {i}/{self.max_iter}, Loss={loss:.6f}\")\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        acts, _ = self._forward_pass(X)\n",
        "        return acts[-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return np.argmax(probs, axis=1)\n",
        "\n",
        "    def score(self, X, y):\n",
        "        return np.mean(self.predict(X)==y)\n",
        "\n",
        "    def save(self, fname=\"mlp_model.npz\"):\n",
        "        np.savez(fname,\n",
        "            weights=np.array(self.weights, dtype=object),\n",
        "            biases=np.array(self.biases,  dtype=object)\n",
        "        )\n",
        "\n",
        "print(\"Starting multi-class MLP training...\")\n",
        "\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(150,),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    max_iter=200,\n",
        "    learning_rate=0.001,\n",
        "    batch_size=32,\n",
        "    random_state=42\n",
        ")\n",
        "mlp.fit(X_train, y_train)\n",
        "train_acc = mlp.score(X_train, y_train)\n",
        "val_acc   = mlp.score(X_val,   y_val)\n",
        "test_acc  = mlp.score(X_test,  y_test)\n",
        "print(\"Train Accuracy:\", train_acc)\n",
        "print(\"Val Accuracy:\",   val_acc)\n",
        "print(\"Test Accuracy:\",  test_acc)\n",
        "mlp.save(\"mlp_model.npz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
